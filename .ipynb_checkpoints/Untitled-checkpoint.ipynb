{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    # path to training data\n",
    "    training_data_path = 'data/conversations_lenmax22_formersents2_with_former'\n",
    "\n",
    "    # path to all_words\n",
    "    all_words_path = 'data/all_words.txt'\n",
    "\n",
    "    # training parameters \n",
    "    CHECKPOINT = True\n",
    "    train_model_path = 'model'\n",
    "    train_model_name = 'model-55'\n",
    "    start_epoch = 56\n",
    "    start_batch = 0\n",
    "    batch_size = 25\n",
    "\n",
    "    # for RL training\n",
    "    training_type = 'normal' # 'normal' for seq2seq training, 'pg' for policy gradient\n",
    "    reversed_model_path = 'Adam_encode22_decode22_reversed-maxlen22_lr0.0001_batch25_wordthres6'\n",
    "    reversed_model_name = 'model-63'\n",
    "\n",
    "    # data reader shuffle index list\n",
    "    load_list = False\n",
    "    index_list_file = 'data/shuffle_index_list'\n",
    "    cur_train_index = start_batch * batch_size\n",
    "\n",
    "    # word count threshold\n",
    "    WC_threshold = 20\n",
    "    reversed_WC_threshold = 6\n",
    "\n",
    "    # dialog simulation turns\n",
    "    MAX_TURNS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 22.192663192749023 secs\n",
      "\n",
      "len conversation 83097\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'L195'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e8d7045dd11d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mcon_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterance_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mcon_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutterance_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcon_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m22\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcon_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'L195'"
     ]
    }
   ],
   "source": [
    "import _pickle as pickle\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "WORD_VECTOR_SIZE = 300\n",
    "\n",
    "raw_movie_conversations = open('data/movie_conversations.txt', 'r').read().split('\\n')[:-1]\n",
    "\n",
    "utterance_dict = pickle.load(open('data/utterance_dict', 'rb'))\n",
    "\n",
    "ts = time.time()\n",
    "corpus = word2vec.Text8Corpus(\"data/tokenized_all_words.txt\")\n",
    "word_vector = word2vec.Word2Vec(corpus, size=WORD_VECTOR_SIZE)\n",
    "word_vector.wv.save_word2vec_format(u\"model/word_vector.bin\", binary=True)\n",
    "word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))\n",
    "\n",
    "\"\"\" Extract only the vocabulary part of the data \"\"\"\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data\n",
    "\n",
    "ts = time.time()\n",
    "conversations = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a = utterance_dict[conversation[i+1]].strip()\n",
    "        con_b = utterance_dict[conversation[i]].strip()\n",
    "        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            traindata_count += 1\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/reversed_conversations_lenmax22', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))\n",
    "\n",
    "# some statistics of training data\n",
    "max_a = -1\n",
    "max_b = -1\n",
    "max_a_ind = -1\n",
    "max_b_ind = -1\n",
    "sum_a = 0.\n",
    "sum_b = 0.\n",
    "len_a_list = []\n",
    "len_b_list = []\n",
    "for i in range(len(conversations)):\n",
    "    len_a = len(conversations[i][0])\n",
    "    len_b = len(conversations[i][1].split())\n",
    "    if len_a > max_a:\n",
    "        max_a = len_a\n",
    "        max_a_ind = i\n",
    "    if len_b > max_b:\n",
    "        max_b = len_b\n",
    "        max_b_ind = i\n",
    "    sum_a += len_a\n",
    "    sum_b += len_b\n",
    "    len_a_list.append(len_a)\n",
    "    len_b_list.append(len_b)\n",
    "np.save(\"data/reversed_lenmax22_a_list\", np.array(len_a_list))\n",
    "np.save(\"data/reversed_lenmax22_b_list\", np.array(len_b_list))\n",
    "print(\"max_a_ind {}, max_b_ind {}\".format(max_a_ind, max_b_ind))\n",
    "print(\"max_a {}, max_b {}, avg_a {}, avg_b {}\".format(max_a, max_b, sum_a/len(conversations), sum_b/len(conversations)))\n",
    "\n",
    "ts = time.time()\n",
    "conversations = []\n",
    "# former_sents = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    con_a_1 = ''\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a_2 = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = \"{} {}\".format(con_a_1, con_a_2)\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b, con_a_2))\n",
    "            # former_sents.append(con_a_2)\n",
    "            traindata_count += 1\n",
    "        con_a_1 = con_a_2\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22_formersents2_with_former', 'wb'), True)\n",
    "# pickle.dump(former_sents, open('data/conversations_lenmax22_former_sents', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))\n",
    "\n",
    "ts = time.time()\n",
    "conversations = []\n",
    "# former_sents = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    con_a_1 = ''\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a_2 = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a_1.split()) <= 22 and len(con_a_2.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = \"{} {}\".format(con_a_1, con_a_2)\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            # former_sents.append(con_a_2)\n",
    "            traindata_count += 1\n",
    "        con_a_1 = con_a_2\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22_former_sents2', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))\n",
    "\n",
    "ts = time.time()\n",
    "conversations = []\n",
    "print('len conversation', len(raw_movie_conversations))\n",
    "con_count = 0\n",
    "traindata_count = 0\n",
    "for conversation in raw_movie_conversations:\n",
    "    conversation = conversation.split(' +++$+++ ')[-1]\n",
    "    conversation = conversation.replace('[', '')\n",
    "    conversation = conversation.replace(']', '')\n",
    "    conversation = conversation.replace('\\'', '')\n",
    "    conversation = conversation.split(', ')\n",
    "    assert len(conversation) > 1\n",
    "    for i in range(len(conversation)-1):\n",
    "        con_a = utterance_dict[conversation[i]]\n",
    "        con_b = utterance_dict[conversation[i+1]]\n",
    "        if len(con_a.split()) <= 22 and len(con_b.split()) <= 22:\n",
    "            con_a = [refine(w) for w in con_a.lower().split()]\n",
    "            # con_a = [word_vector[w] if w in word_vector else np.zeros(WORD_VECTOR_SIZE) for w in con_a]\n",
    "            conversations.append((con_a, con_b))\n",
    "            traindata_count += 1\n",
    "    con_count += 1\n",
    "    if con_count % 1000 == 0:\n",
    "        print('con_count {}, traindata_count {}'.format(con_count, traindata_count))\n",
    "pickle.dump(conversations, open('data/conversations_lenmax22', 'wb'), True)\n",
    "print(\"Time Elapsed: {} secs\\n\".format(time.time() - ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'L195'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-cdeba997c02c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutterance_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'L195'"
     ]
    }
   ],
   "source": [
    "utterance_dict[conversation[i+1]].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as Pickle\n",
    "import random\n",
    "\n",
    "class Data_Reader:\n",
    "    def __init__(self, cur_train_index=0, load_list=False):\n",
    "        self.training_data = pickle.load(open(config.training_data_path, 'rb'))\n",
    "        self.data_size = len(self.training_data)\n",
    "        if load_list:\n",
    "            self.shuffle_list = pickle.load(open(config.index_list_file, 'rb'))\n",
    "        else:    \n",
    "            self.shuffle_list = self.shuffle_index()\n",
    "        self.train_index = cur_train_index\n",
    "\n",
    "    def get_batch_num(self, batch_size):\n",
    "        return self.data_size // batch_size\n",
    "\n",
    "    def shuffle_index(self):\n",
    "        shuffle_index_list = random.sample(range(self.data_size), self.data_size)\n",
    "        pickle.dump(shuffle_index_list, open(config.index_list_file, 'wb'), True)\n",
    "        return shuffle_index_list\n",
    "\n",
    "    def generate_batch_index(self, batch_size):\n",
    "        if self.train_index + batch_size > self.data_size:\n",
    "            batch_index = self.shuffle_list[self.train_index:self.data_size]\n",
    "            self.shuffle_list = self.shuffle_index()\n",
    "            remain_size = batch_size - (self.data_size - self.train_index)\n",
    "            batch_index += self.shuffle_list[:remain_size]\n",
    "            self.train_index = remain_size\n",
    "        else:\n",
    "            batch_index = self.shuffle_list[self.train_index:self.train_index+batch_size]\n",
    "            self.train_index += batch_size\n",
    "\n",
    "        return batch_index\n",
    "\n",
    "    def generate_training_batch(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n",
    "\n",
    "        return batch_X, batch_Y\n",
    "\n",
    "    def generate_training_batch_with_former(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "        batch_Y = [self.training_data[i][1] for i in batch_index]   # batch_size of conv_b\n",
    "        former = [self.training_data[i][2] for i in batch_index]    # batch_size of former utterance\n",
    "\n",
    "        return batch_X, batch_Y, former\n",
    "\n",
    "    def generate_testing_batch(self, batch_size):\n",
    "        batch_index = self.generate_batch_index(batch_size)\n",
    "        batch_X = [self.training_data[i][0] for i in batch_index]   # batch_size of conv_a\n",
    "\n",
    "        return batch_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def preProBuildWordVocab(word_count_threshold=5, all_words_path=config.all_words_path):\n",
    "    # borrowed this function from NeuralTalk\n",
    "\n",
    "    if not os.path.exists(all_words_path):\n",
    "        parse_all_words(all_words_path)\n",
    "\n",
    "    corpus = open(all_words_path, 'r').read().split('\\n')[:-1]\n",
    "    captions = np.asarray(corpus, dtype=np.object)\n",
    "\n",
    "    captions = map(lambda x: x.replace('.', ''), captions)\n",
    "    captions = map(lambda x: x.replace(',', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "    captions = map(lambda x: x.replace('?', ''), captions)\n",
    "    captions = map(lambda x: x.replace('!', ''), captions)\n",
    "    captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "    captions = map(lambda x: x.replace('/', ''), captions)\n",
    "\n",
    "    print('preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold))\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in captions:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "           word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print('filtered words from %d to %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '<pad>'\n",
    "    ixtoword[1] = '<bos>'\n",
    "    ixtoword[2] = '<eos>'\n",
    "    ixtoword[3] = '<unk>'\n",
    "\n",
    "    wordtoix = {}\n",
    "    wordtoix['<pad>'] = 0\n",
    "    wordtoix['<bos>'] = 1\n",
    "    wordtoix['<eos>'] = 2\n",
    "    wordtoix['<unk>'] = 3\n",
    "\n",
    "    for idx, w in enumerate(vocab):\n",
    "        wordtoix[w] = idx+4\n",
    "        ixtoword[idx+4] = w\n",
    "\n",
    "    word_counts['<pad>'] = nsents\n",
    "    word_counts['<bos>'] = nsents\n",
    "    word_counts['<eos>'] = nsents\n",
    "    word_counts['<unk>'] = nsents\n",
    "\n",
    "    bias_init_vector = np.array([1.0 * word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector\n",
    "\n",
    "def parse_all_words(all_words_path):\n",
    "    raw_movie_lines = open('data/all_words.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n",
    "\n",
    "    with codecs.open(all_words_path, \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        for line in raw_movie_lines:\n",
    "            line = line.split(' +++$+++ ')\n",
    "            utterance = line[-1]\n",
    "            f.write(utterance + '\\n')\n",
    "\n",
    "\"\"\" Extract only the vocabulary part of the data \"\"\"\n",
    "def refine(data):\n",
    "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
    "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
    "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
    "    data = ' '.join(words)\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parse_all_words(config.all_words_path)\n",
    "\n",
    "    raw_movie_lines = open('data/all_words.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')[:-1]\n",
    "    \n",
    "    utterance_dict = {}\n",
    "    with codecs.open('data/tokenized_all_words.txt', \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        for line in raw_movie_lines:\n",
    "            line = line.split(' +++$+++ ')\n",
    "            line_ID = line[0]\n",
    "            utterance = line[-1]\n",
    "            utterance_dict[line_ID] = utterance\n",
    "            utterance = \" \".join([refine(w) for w in utterance.lower().split()])\n",
    "            f.write(utterance + '\\n')\n",
    "    pickle.dump(utterance_dict, open('data/utterance_dict', 'wb'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Seq2Seq_chatbot():\n",
    "    def __init__(self, dim_wordvec, n_words, dim_hidden, batch_size, n_encode_lstm_step, n_decode_lstm_step, bias_init_vector=None, lr=0.0001):\n",
    "        self.dim_wordvec = dim_wordvec\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_words = n_words\n",
    "        self.n_encode_lstm_step = n_encode_lstm_step\n",
    "        self.n_decode_lstm_step = n_decode_lstm_step\n",
    "        self.lr = lr\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name='Wemb')\n",
    "\n",
    "        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        self.encode_vector_W = tf.Variable(tf.random_uniform([dim_wordvec, dim_hidden], -0.1, 0.1), name='encode_vector_W')\n",
    "        self.encode_vector_b = tf.Variable(tf.zeros([dim_hidden]), name='encode_vector_b')\n",
    "\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='embed_word_W')\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name='embed_word_b')\n",
    "\n",
    "    def build_model(self):\n",
    "        word_vectors = tf.placeholder(tf.float32, [self.batch_size, self.n_encode_lstm_step, self.dim_wordvec])\n",
    "\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_decode_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_decode_lstm_step+1])\n",
    "\n",
    "        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n",
    "        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b ) # (batch_size*n_encode_lstm_step, dim_hidden)\n",
    "        wordvec_emb = tf.reshape(wordvec_emb, [self.batch_size, self.n_encode_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "\n",
    "        probs = []\n",
    "        entropies = []\n",
    "        loss = 0.0\n",
    "\n",
    "        ##############################  Encoding Stage ##################################\n",
    "        for i in range(0, self.n_encode_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        ############################# Decoding Stage ######################################\n",
    "        for i in range(0, self.n_decode_lstm_step):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels], 1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:, i]\n",
    "            entropies.append(cross_entropy)\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "            train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        inter_value = {\n",
    "            'probs': probs,\n",
    "            'entropies': entropies\n",
    "        }\n",
    "\n",
    "        return train_op, loss, word_vectors, caption, caption_mask, inter_value\n",
    "\n",
    "    def build_generator(self):\n",
    "        word_vectors = tf.placeholder(tf.float32, [1, self.n_encode_lstm_step, self.dim_wordvec])\n",
    "\n",
    "        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n",
    "        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b)\n",
    "        wordvec_emb = tf.reshape(wordvec_emb, [1, self.n_encode_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "        padding = tf.zeros([1, self.dim_hidden])\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        probs = []\n",
    "        embeds = []\n",
    "\n",
    "        for i in range(0, self.n_encode_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        for i in range(0, self.n_decode_lstm_step):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([1], dtype=tf.int64))\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "            generated_words.append(max_prob_index)\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n",
    "                current_embed = tf.expand_dims(current_embed, 0)\n",
    "\n",
    "            embeds.append(current_embed)\n",
    "\n",
    "        return word_vectors, generated_words, probs, embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Seq2Seq_chatbot():\n",
    "    def __init__(self, dim_wordvec, n_words, dim_hidden, batch_size, n_encode_lstm_step, n_decode_lstm_step, bias_init_vector=None, lr=0.0001):\n",
    "        self.dim_wordvec = dim_wordvec\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_words = n_words\n",
    "        self.n_encode_lstm_step = n_encode_lstm_step\n",
    "        self.n_decode_lstm_step = n_decode_lstm_step\n",
    "        self.lr = lr\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_hidden], -0.1, 0.1), name='Wemb')\n",
    "\n",
    "        self.lstm1 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "        self.lstm2 = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        self.encode_vector_W = tf.Variable(tf.random_uniform([dim_wordvec, dim_hidden], -0.1, 0.1), name='encode_vector_W')\n",
    "        self.encode_vector_b = tf.Variable(tf.zeros([dim_hidden]), name='encode_vector_b')\n",
    "\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='embed_word_W')\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = tf.Variable(tf.zeros([n_words]), name='embed_word_b')\n",
    "\n",
    "    def build_model(self):\n",
    "        word_vectors = tf.placeholder(tf.float32, [self.batch_size, self.n_encode_lstm_step, self.dim_wordvec])\n",
    "\n",
    "        caption = tf.placeholder(tf.int32, [self.batch_size, self.n_decode_lstm_step+1])\n",
    "        caption_mask = tf.placeholder(tf.float32, [self.batch_size, self.n_decode_lstm_step+1])\n",
    "\n",
    "        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n",
    "        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b ) # (batch_size*n_encode_lstm_step, dim_hidden)\n",
    "        wordvec_emb = tf.reshape(wordvec_emb, [self.batch_size, self.n_encode_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([self.batch_size, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([self.batch_size, self.lstm2.state_size])\n",
    "        padding = tf.zeros([self.batch_size, self.dim_hidden])\n",
    "\n",
    "        probs = []\n",
    "        entropies = []\n",
    "        loss = 0.0\n",
    "\n",
    "        ##############################  Encoding Stage ##################################\n",
    "        for i in range(0, self.n_encode_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        ############################# Decoding Stage ######################################\n",
    "        for i in range(0, self.n_decode_lstm_step):\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, caption[:, i])\n",
    "\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            labels = tf.expand_dims(caption[:, i+1], 1)\n",
    "            indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "            concated = tf.concat([indices, labels], 1)\n",
    "            onehot_labels = tf.sparse_to_dense(concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
    "            cross_entropy = cross_entropy * caption_mask[:, i]\n",
    "            entropies.append(cross_entropy)\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            current_loss = tf.reduce_sum(cross_entropy)/self.batch_size\n",
    "            loss = loss + current_loss\n",
    "\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "            train_op = tf.train.GradientDescentOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "        inter_value = {\n",
    "            'probs': probs,\n",
    "            'entropies': entropies\n",
    "        }\n",
    "\n",
    "        return train_op, loss, word_vectors, caption, caption_mask, inter_value\n",
    "\n",
    "    def build_generator(self):\n",
    "        word_vectors = tf.placeholder(tf.float32, [1, self.n_encode_lstm_step, self.dim_wordvec])\n",
    "\n",
    "        word_vectors_flat = tf.reshape(word_vectors, [-1, self.dim_wordvec])\n",
    "        wordvec_emb = tf.nn.xw_plus_b(word_vectors_flat, self.encode_vector_W, self.encode_vector_b)\n",
    "        wordvec_emb = tf.reshape(wordvec_emb, [1, self.n_encode_lstm_step, self.dim_hidden])\n",
    "\n",
    "        state1 = tf.zeros([1, self.lstm1.state_size])\n",
    "        state2 = tf.zeros([1, self.lstm2.state_size])\n",
    "        padding = tf.zeros([1, self.dim_hidden])\n",
    "\n",
    "        generated_words = []\n",
    "\n",
    "        probs = []\n",
    "        embeds = []\n",
    "\n",
    "        for i in range(0, self.n_encode_lstm_step):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(wordvec_emb[:, i, :], state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([padding, output1], 1), state2)\n",
    "\n",
    "        for i in range(0, self.n_decode_lstm_step):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            if i == 0:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    current_embed = tf.nn.embedding_lookup(self.Wemb, tf.ones([1], dtype=tf.int64))\n",
    "\n",
    "            with tf.variable_scope(\"LSTM1\"):\n",
    "                output1, state1 = self.lstm1(padding, state1)\n",
    "\n",
    "            with tf.variable_scope(\"LSTM2\"):\n",
    "                output2, state2 = self.lstm2(tf.concat([current_embed, output1], 1), state2)\n",
    "\n",
    "            logit_words = tf.nn.xw_plus_b(output2, self.embed_word_W, self.embed_word_b)\n",
    "            max_prob_index = tf.argmax(logit_words, 1)[0]\n",
    "            generated_words.append(max_prob_index)\n",
    "            probs.append(logit_words)\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                current_embed = tf.nn.embedding_lookup(self.Wemb, max_prob_index)\n",
    "                current_embed = tf.expand_dims(current_embed, 0)\n",
    "\n",
    "            embeds.append(current_embed)\n",
    "\n",
    "        return word_vectors, generated_words, probs, embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing word counts and creating vocab based on word count threshold 20\n",
      "filtered words from 76029 to 6847\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000002A8A01F0748>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000002A8A01F07F0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "Restart training...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/conversations_lenmax22_formersents2_with_former'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7a0fbb06680d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-7a0fbb06680d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mdr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_Reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5d421cbf8be5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cur_train_index, load_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mData_Reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_train_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data/conversations_lenmax22_formersents2_with_former'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "### Global Parameters ###\n",
    "checkpoint = config.CHECKPOINT\n",
    "model_path = config.train_model_path\n",
    "model_name = config.train_model_name\n",
    "start_epoch = config.start_epoch\n",
    "\n",
    "word_count_threshold = config.WC_threshold\n",
    "\n",
    "### Train Parameters ###\n",
    "dim_wordvec = 300\n",
    "dim_hidden = 1000\n",
    "\n",
    "n_encode_lstm_step = 22 + 22\n",
    "n_decode_lstm_step = 22\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "def train():\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
    "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
    "\n",
    "    model = Seq2Seq_chatbot(\n",
    "            dim_wordvec=dim_wordvec,\n",
    "            n_words=len(wordtoix),\n",
    "            dim_hidden=dim_hidden,\n",
    "            batch_size=batch_size,\n",
    "            n_encode_lstm_step=n_encode_lstm_step,\n",
    "            n_decode_lstm_step=n_decode_lstm_step,\n",
    "            bias_init_vector=bias_init_vector,\n",
    "            lr=learning_rate)\n",
    "\n",
    "    train_op, tf_loss, word_vectors, tf_caption, tf_caption_mask, inter_value = model.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    #save_path = saver.save(sess, \"/model/model.ckpt\")\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "#    if checkpoint:\n",
    "#        print(\"Use Model {}.\".format(model_name))\n",
    "#        saver.restore(sess, os.path.join(model_path, model_name))\n",
    "#        print(\"Model {} restored.\".format(model_name))\n",
    "#    else:\n",
    "    print(\"Restart training...\")\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    dr = Data_Reader()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        n_batch = dr.get_batch_num(batch_size)\n",
    "        for batch in range(n_batch):\n",
    "            start_time = time.time()\n",
    "\n",
    "            batch_X, batch_Y = dr.generate_training_batch(batch_size)\n",
    "\n",
    "            for i in range(len(batch_X)):\n",
    "                batch_X[i] = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in batch_X[i]]\n",
    "                # batch_X[i].insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n",
    "                if len(batch_X[i]) > n_encode_lstm_step:\n",
    "                    batch_X[i] = batch_X[i][:n_encode_lstm_step]\n",
    "                else:\n",
    "                    for _ in range(len(batch_X[i]), n_encode_lstm_step):\n",
    "                        batch_X[i].append(np.zeros(dim_wordvec))\n",
    "\n",
    "            current_feats = np.array(batch_X)\n",
    "\n",
    "            current_captions = batch_Y\n",
    "            current_captions = map(lambda x: '<bos> ' + x, current_captions)\n",
    "            current_captions = map(lambda x: x.replace('.', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace(',', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\"', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\\n', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('?', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('!', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('\\\\', ''), current_captions)\n",
    "            current_captions = map(lambda x: x.replace('/', ''), current_captions)\n",
    "\n",
    "            for idx, each_cap in enumerate(current_captions):\n",
    "                word = each_cap.lower().split(' ')\n",
    "                if len(word) < n_decode_lstm_step:\n",
    "                    current_captions[idx] = current_captions[idx] + ' <eos>'\n",
    "                else:\n",
    "                    new_word = ''\n",
    "                    for i in range(n_decode_lstm_step-1):\n",
    "                        new_word = new_word + word[i] + ' '\n",
    "                    current_captions[idx] = new_word + '<eos>'\n",
    "\n",
    "            current_caption_ind = []\n",
    "            for cap in current_captions:\n",
    "                current_word_ind = []\n",
    "                for word in cap.lower().split(' '):\n",
    "                    if word in wordtoix:\n",
    "                        current_word_ind.append(wordtoix[word])\n",
    "                    else:\n",
    "                        current_word_ind.append(wordtoix['<unk>'])\n",
    "                current_caption_ind.append(current_word_ind)\n",
    "\n",
    "            current_caption_matrix = pad_sequences(current_caption_ind, padding='post', maxlen=n_decode_lstm_step)\n",
    "            current_caption_matrix = np.hstack([current_caption_matrix, np.zeros([len(current_caption_matrix), 1])]).astype(int)\n",
    "            current_caption_masks = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array(map(lambda x: (x != 0).sum() + 1, current_caption_matrix))\n",
    "\n",
    "            for ind, row in enumerate(current_caption_masks):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                _, loss_val = sess.run(\n",
    "                        [train_op, tf_loss],\n",
    "                        feed_dict={\n",
    "                            word_vectors: current_feats,\n",
    "                            tf_caption: current_caption_matrix,\n",
    "                            tf_caption_mask: current_caption_masks\n",
    "                        })\n",
    "                print(\"Epoch: {}, batch: {}, loss: {}, Elapsed time: {}\".format(epoch, batch, loss_val, time.time() - start_time))\n",
    "            else:\n",
    "                _ = sess.run(train_op,\n",
    "                             feed_dict={\n",
    "                                word_vectors: current_feats,\n",
    "                                tf_caption: current_caption_matrix,\n",
    "                                tf_caption_mask: current_caption_masks\n",
    "                            })\n",
    "\n",
    "\n",
    "        print(\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HIRA_3",
   "language": "python",
   "name": "hira_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
